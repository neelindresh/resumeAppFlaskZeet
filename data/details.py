ABOUT_ME={
    "position":"Data Scientist | Author PyMLPipe",
    "about": "I have extensive experience delivering end-to-end analytics solutions, having successfully completed over 15 projects across various industries. My expertise lies in leveraging data to uncover insights, drive business outcomes, and create value for organizations. I am a strategic thinker with excellent problem-solving skills, and I have a proven track record of delivering innovative solutions that meet and exceed expectations. I am eager to bring my expertise and passion for data science to a dynamic and forward-thinking organization.",
    "intro": "A self-motivated, tech-savvy and result oriented professional who always looks for opportunity to exercise creative and innovative ideas to make contribution to the development of the organization. Experienced in creating end-to- end Analytics solutions.",
    
}

ABOUT_DETAILS=[
    {"exp":"05+ Years", "icon":"uil uil-briefcase-alt","title":"Experience"},
    {"exp":"15+ Completed",'icon':"uil uil-apps","title":"Projects"},
    {"exp":"10+ Client",'icon':"uil uil-constructor","title":"Happy"}
]

SKILLS=[
    {
        "header":"Programming",
        'target':"coding",
        "icon":"uil uil-code-branch",
        "exp":"05+",
        "tools":[
            {
                    "name":"Python",
                    "perct":90,
            },
            {
                    "name":"C",
                    "perct":60,
            },
            {
                    "name":"C++",
                    "perct":60,
            },
            {
                    "name":"Java",
                    "perct":50,
            },
            
        ]
    },
    {
        "header":"Machine Learning",
        'target':"ml",
        'icon':"uil uil-robot",
        "exp":"05+",
        "tools":[
            {
                    "name":"Scikit-learn",
                    "perct":90,
            },
            {
                    "name":"Pandas",
                    "perct":70,
            },
            {
                    "name":"Numpy",
                    "perct":70,
            },
            {
                    "name":"Predictive Modeling",
                    "perct":80,
            },
            {
                    "name":"Statistical Modeling",
                    "perct":80,
            },
            {
                    "name":"Forcasting",
                    "perct":60,
            },
            {
                    "name":"Supervised Learning",
                    "perct":90,
            },
            {
                    "name":"Unsupervised Learning",
                    "perct":80,
            },
        ]
    },
    {
        "header":"Deep Learning",
        'target':"dl",
        "icon":"uil uil-channel-add",
        "exp":"05+",
        "tools":[
            {
                    "name":"Pytorch",
                    "perct":90,
            },
            {
                    "name":"Tensorflow",
                    "perct":50,
            },
            {
                    "name":"Computer Vision (CV)",
                    "perct":70,
            },
            {
                    "name":"Natural Language Processing(NLP)",
                    "perct":80,
            },
            {
                    "name":"NLU",
                    "perct":80,
            },
            {
                    "name":"Generative Modeling ",
                    "perct":60,
            },
            
        ]
    },
    {
        "header":"Data Orchestration",
        'target':"cloud",
        "icon":"uil uil-cloud-computing",
        "exp":"05+",
        "tools":[
            {
                    "name":"Airflow",
                    "perct":70,
            },
            {
                    "name":"Docker",
                    "perct":80,
            },
            {
                    "name":"PySpark",
                    "perct":60,
            },
            {
                    "name":"MLOps",
                    "perct":90,
            },
            {
                    "name":"NoSQL",
                    "perct":80,
            },
            {
                    "name":"SQL",
                    "perct":60,
            },
            
        ]
    }
    
]

QUALIFICATION={
        "experience":[
                        {
                                "organization":"KPMG India",
                                "title":"Consultant",
                                "years":"2021-present"
                        },
                        {
                                "organization":"PRM Fincon",
                                "title":"Data Scientist",
                                "years":"2020-2021"
                        },
                        {
                                "organization":"Virtusa",
                                "title":"ASSOCIATE ML ENGINEER | ML ENGINEER",
                                "years":"2018-2020"
                        }
                ],
        "education":[
                        {
                                "organization":"Vellore Institute of Technology(VIT)",
                                "title":"Master of Compter Application: Data Science",
                                "years":"2016-2018"
                        },
                        {
                                "organization":"West Bengal State univercity",
                                "title":"Bachelor of Science: Computer Science",
                                "years":"2013-2016"
                        },
                        
                ],
}


    
PROJECTS=[
         {
                "title":"Financial document Classification",
                "desc":"Financial document classification refers to the process of automatically categorizing financial documents into different classes or categories based on their contents. This can be achieved using techniques such as Machine Learning, Natural Language Processing (NLP) and Computer Vision. The main objective of financial document classification is to help organizations in better organizing their financial data, reducing manual effort and improving efficiency. The classification can be performed on various financial documents such as balance sheets, income statements, invoices, receipts, etc. The result of the classification process can be used for various tasks such as financial statement analysis, fraud detection, and regulatory compliance. Financial document classification using AI can help auditors by automating the process of categorizing financial documents into relevant categories, reducing manual effort and increasing efficiency. This can help auditors in identifying and organizing relevant documents for the audit process, saving time and effort. The use of AI can also reduce the risk of errors in document classification, improving the overall accuracy and reliability of the audit process.",
                "keypoints":['Data Collection: Gather a diverse set of financial documents that need to be classified and organize them into different categories or classes.', 'Data Pre-processing: Clean and pre-process the data to remove any irrelevant information or formatting issues.', 'Feature Engineering: Extract relevant features from the financial documents that can be used as inputs for the model.', 'Model Selection: Choose an appropriate machine learning model for the classification task.', "Model Training: Train the model on the pre-processed data and fine-tune the model's hyperparameters to optimize its performance.", 'Model Evaluation: Evaluate the performance of the model using appropriate metrics and make any necessary changes to improve its accuracy.', 'Deployment: Deploy the model into a production environment and integrate it with any necessary tools or platforms.'],
                "role":"Data Scientist",
                "org": "PRM Fincon",
                "tech": ["python","PyTorch","Deep Learning" ,"NLP","Computer Vision","OCR","API"],
                "Links":"",
                "category":"DL",
                "year": 2020
        },
        {
                "title":"Financial document extraction",
                "desc":"Financial document extraction is a process that uses AI and NLP techniques to automatically extract relevant information from financial documents such as invoices, receipts, financial statements, and tax forms. To build the model, various machine learning algorithms, natural language processing (NLP) techniques, and computer vision may be used. Once the model has been developed, it can be integrated into a financial document management system or accounting software to automate the process of extracting financial information. AI can greatly assist auditors in the review and analysis of financial records, saving time and increasing accuracy. It can also reduce the risk of human error and increase the speed and efficiency of the auditing process. Overall, this technology can provide valuable support for auditors in their efforts to ensure the accuracy and integrity of financial records. The extracted information can help auditors in their financial statement analysis and reporting tasks. Various tools such as Optical Character Recognition (OCR) and Natural Language Processing (NLP) can be used to perform financial document extraction.",
                "keypoints":['Data Collection: To build an effective model, a large and diverse dataset of financial documents is required. This dataset will be used to train the model on how to identify and extract relevant information from financial documents.', "Data Cleaning and Preprocessing: The collected data may contain noise, irrelevant information, and inconsistent data structures. To improve the model's accuracy, the data must be cleaned and preprocessed to standardize the data format and remove any irrelevant information.", 'Feature Engineering: Feature engineering is the process of creating features that the model can use to identify and extract relevant information from financial documents. This may include creating features such as keywords, phrases, entity recognition, and image recognition features.', 'Model Development: Based on the preprocessed data, machine learning algorithms and NLP techniques are used to develop a model that can automatically extract relevant information from financial documents. This may involve using deep learning algorithms, such as convolutional neural networks (CNNs) or recurrent neural networks (RNNs), or traditional machine learning algorithms such as decision trees or support vector machines (SVMs).', 'Model Validation and Testing: To ensure that the model is accurate and reliable, it must be tested on a separate dataset of financial documents. This will help to identify any errors in the model and ensure that it is working as expected.', 'Integration and Deployment: Once the model has been validated, it can be integrated into a financial document management system or accounting software to automate the process of extracting financial information from financial documents. The model must also be deployed in a scalable and secure environment, to ensure that it can handle the volume of financial documents that it will be processing.'],
                "role":"Data Scientist",
                "org": "PRM Fincon",
                "tech": ["python","PyTorch","Deep Learning" ,"NLP","Computer Vision","OCR","API"],
                "Links":"",
                "category":"DL",
                "year": 2020
        },
        {
                "title":"Automatic Financial document tagging",
                "desc":"Automatic document tagging for financial documents involves using AI and NLP techniques to analyze and categorize financial documents into relevant tags. The goal of this project is to streamline the process of organizing financial documents and make it more efficient and accurate. The project involves developing a model that can automatically identify and extract key information from financial documents and assign relevant tags based on the content. Tools such as machine learning algorithms, natural language processing, and computer vision may be used to build the model. The project may also involve integrating the model into a larger financial document management system to improve the overall document organization process. \n n the context of financial documents, this process involves extracting key information from these documents and assigning relevant tags based on the content. The goal is to make the process of organizing financial documents more efficient and accurate. \n To build the model, various machine learning algorithms, natural language processing (NLP) techniques, and computer vision may be used. For example, NLP techniques can be used to identify and extract relevant information from the text of the financial documents, while computer vision can be used to extract information from any images or tables in the document. \n Once the model has been developed, it can be integrated into a larger financial document management system to improve the overall document organization process. This can help reduce the time and effort required to manually categorize and tag financial documents, and ensure that the process is more accurate and consistent      ",
                "keypoints":['Data Collection: The first step is to collect a dataset of images, videos, and text-based content that will be used to train and test the content classification model. This dataset should include examples of both appropriate and inappropriate content.', 'Data Pre-processing: Before building the model, the dataset needs to be pre-processed to remove any irrelevant or redundant information, and to format the data in a way that is suitable for use with machine learning algorithms.', 'Feature Engineering: Next, the relevant features of the content need to be extracted and represented in a form that can be used by the model. This may involve applying computer vision techniques to extract features from images and videos, or NLP techniques to extract features from text-based content.', 'Model Training: The next step is to train a deep learning model using the pre-processed and feature-engineered dataset. This typically involves using algorithms such as convolutional neural networks (CNNs) for image classification or recurrent neural networks (RNNs) for text classification.', "Model Validation: After training the model, it is important to evaluate its performance to ensure that it is accurate and reliable. This can be done by using a validation dataset and comparing the model's predictions with the ground truth labels.", 'Deployment: Finally, once the model has been validated and shown to be accurate, it can be deployed in production to automatically classify new content as appropriate or not appropriate, based on its content.'],
                "role":"Data Scientist",
                "org": "PRM Fincon",
                "tech": ["python","PyTorch","Deep Learning" ,"NLP","Computer Vision",],
                "Links":"",
                "category":"DL",
                "year": 2020
        },
        {
                "title":"Google Ads Validation framework",
                "desc":"Classifying not family-friendly content in Google Ads using NLP (Natural Language Processing) and computer vision involves using a combination of these technologies to identify and filter out inappropriate content. NLP techniques can be used to analyze text-based content, such as ad descriptions, to identify words and phrases that are commonly associated with inappropriate content. Meanwhile, computer vision techniques can be used to analyze images and videos, identifying objects and scenes that are inappropriate. These technologies can be combined to provide a comprehensive approach to content classification, which can accurately and efficiently identify inappropriate content and prevent it from being displayed in Google Ads. The goal is to ensure that all content displayed is safe and suitable for the intended audience.",
                "keypoints":['Data Collection: The first step is to collect a dataset of images, videos, and text-based content that will be used to train and test the content classification model. This dataset should include examples of both appropriate and inappropriate content.', 'Data Pre-processing: Before building the model, the dataset needs to be pre-processed to remove any irrelevant or redundant information, and to format the data in a way that is suitable for use with machine learning algorithms.', 'Feature Engineering: Next, the relevant features of the content need to be extracted and represented in a form that can be used by the model. This may involve applying computer vision techniques to extract features from images and videos, or NLP techniques to extract features from text-based content.', 'Model Training: The next step is to train a deep learning model using the pre-processed and feature-engineered dataset. This typically involves using algorithms such as convolutional neural networks (CNNs) for image classification or recurrent neural networks (RNNs) for text classification.', "Model Validation: After training the model, it is important to evaluate its performance to ensure that it is accurate and reliable. This can be done by using a validation dataset and comparing the model's predictions with the ground truth labels.", 'Deployment: Finally, once the model has been validated and shown to be accurate, it can be deployed in production to automatically classify new content as appropriate or not appropriate, based on its content.'],
                "role":"Machine Learning Engineer",
                "org": "Virtusa",
                "tech": ["python","PyTorch","Deep Learning" ,"spaCy", "NLTK", "Gensim","Machine Learning","NLP","Computer Vision"],
                "Links":"",
                "category":"DL",
                "year":2019
        },
        {
                "title":"GEC Google Ads",
                "desc":"A grammar error correction platform using deep learning for writing Google Ads is a specialized system designed to help marketers and advertisers improve the quality and accuracy of their written ad copy specifically for Google Ads. This platform leverages deep learning algorithms to detect and correct grammatical errors in written text, with a focus on optimizing ad copy for Google's ad platform. The platform uses advanced NLP techniques, such as recurrent neural networks (RNNs) and transformer models, to process and analyze written text, identify grammatical errors, and suggest corrections that are tailored to meet Google's ad copy standards. This specialized grammar error correction platform can help advertisers create more effective and efficient ad copy, ultimately improving the performance and return on investment of their Google Ads campaigns.",
                "keypoints":['Data Collection: The first step is to gather and pre-process a large dataset of text that contains grammatical errors, along with their corrected versions. This dataset will be used to train the deep learning model.', 'Data Pre-processing: The collected text data needs to be cleaned and pre-processed to prepare it for use in the deep learning model. This step includes tasks such as removing special characters, converting text to lowercase, and tokenizing the text into words and sentences.', 'Model Design: Next, design the deep learning model architecture. This can include choosing the type of deep learning algorithms such as RNNs, LSTM networks, or transformer models, determining the number of hidden layers, and defining the size of the input layer.', 'Model Training: Train the deep learning model on the pre-processed data. This involves feeding the model the text data, along with the corrected versions, and updating the model weights and biases to minimize the error between the model predictions and the actual corrected text.', 'Model Evaluation: Evaluate the performance of the deep learning model using accuracy metrics such as precision, recall, and F1 score. This step is also used to identify and address overfitting, which is when the model has learned to make predictions based on the training data, but has poor generalization to new, unseen data.', 'Model Deployment: Once the model is trained and evaluated, it can be deployed to the platform and integrated with Google Ads.', 'Continuous Improvement: Finally, monitor the performance of the model over time and make improvements as necessary. This can include retraining the model on new data, fine-tuning hyperparameters, or updating the model architecture.'],
                "role":"Machine Learning Engineer",
                "org": "Virtusa",
                "tech": ["python","PyTorch","Deep Learning","scikit-learn" ,"spaCy", "NLTK", "Gensim","NLP","RNNs","LSTM ","Transformer"],
                "Links":"",
                "category":"DL",
                "year":2019
        },
        {
                "title":"DQ2, Data Quality Platform",
                "desc":"A data quality platform is a system designed to monitor, analyze, and improve the quality of data. It typically includes features such as data cleansing, data enrichment, data matching, and data reconciliation to improve the accuracy and completeness of the data. It can be critical for organizations that rely on data for decision making, as poor quality data can lead to incorrect insights and decisions. AI-powered data quality platforms can handle large volumes of data in real-time, automate manual and time-consuming tasks, and make the process more efficient and effective. These platforms are becoming increasingly important for organizations relying on data for critical business decisions, as they help to ensure that the data is accurate, consistent, and reliable.",
                "keypoints":[],
                "role":"Machine Learning Engineer",
                "org": "Virtusa",
                "tech": ["python","PyTorch","scikit-learn" ,"spaCy", "NLTK", "Gensim","Machine Learning","NLP"],
                "Links":"",
                "category":"ML",
                "year":2019
        },
        {
               "title":"AI-celerate, GUI based AI platform",
                "desc":"An AI platform is a comprehensive system that enables organizations to develop, deploy, and manage artificial intelligence models and applications. It provides an end-to-end solution for building, training, and deploying machine learning models, and it often includes features such as data management, model development tools, and deployment infrastructure. The goal of an AI platform is to make it easier for organizations to incorporate AI into their operations and workflows, and to help manage the complexity of large-scale AI deployments.",
                "keypoints":["Research and Define Requirements: Conduct market research and gather requirements from potential users to determine what features and functionality the AI platform should have.",
                             "Develop Data Management System: Design and develop a data management system that can efficiently collect, store, and process large amounts of data.",
                             "Create Model Development Tools: Develop tools and libraries that make it easy for users to build, test, and iterate on machine learning models.",
                             "Deployment Infrastructure: Design and implement a deployment infrastructure that enables models to be deployed in a variety of environments, such as cloud, on-premises, or edge devices.",
                             "Monitor and Manage Models: Develop tools for monitoring and managing models in production, including performance and accuracy metrics, to ensure they continue to deliver value over time."],
                "role":"Machine Learning Engineer",
                "org": "Virtusa",
                "tech": ["python","scikit-learn","Machine Learning","flask","Tabular Data","Visualization"],
                "Links":"",
                "category":"ML",
                "year":2018
        },
        {
                "title":"Synthetic image generation",
                "desc":"Synthetic image generation for medical analysis using Generative Adversarial Networks (GANs) is a process of creating artificial medical images using machine learning algorithms. GANs are a type of deep learning neural network that can generate images that are indistinguishable from real images. In medical analysis, synthetic images can be generated to supplement real images and provide additional data for training machine learning models. This is especially useful in cases where there is a shortage of real medical images, such as rare diseases, or when it is not feasible to obtain real images, such as in ethical or legal restrictions. The generated images can be used to train algorithms for medical image analysis tasks, such as segmentation, classification, and detection. The quality of the generated images has improved significantly in recent years, making it possible to generate high-quality images that can support accurate and reliable medical analysis.",
                "keypoints":[],
                "role":"Machine Learning Engineer",
                "org": "Virtusa",
                "tech": ["python","PyTorch","Deep Learning","OpenCV","scikit-image","Pillow","Computer Vision","GAN"],
                "Links":"",
                "category":"DL",
                "year":2018
        },
        {
               "title":"Data Lineage toolbox",
                "desc":"A data lineage tool is a software solution that helps organizations track and manage the flow of data through their systems. The main purpose of a data lineage tool is to provide a clear understanding of how data is transformed and used over time, from its origin to its final destination. This helps organizations to meet regulatory requirements, improve data governance, and ensure data quality. A data lineage tool typically provides visual representations of data flows, allows users to trace data lineage from source to target, and tracks metadata, such as data definitions, business rules, and data transformations. It also provides auditing and reporting capabilities to support compliance and risk management initiatives.",
                "keypoints":['Data Flow Visualization: A visual representation of the flow of data within an organization, including the relationships between data sources, data transformations, and data destinations.', 'Data Traceability: The ability to trace data from its origin to its final destination, providing a clear understanding of how data is used and transformed over time.', 'Metadata Management: The ability to capture, store, and manage metadata, including data definitions, business rules, and data transformations.', 'Impact Analysis: The ability to analyze the impact of changes to data sources, data transformations, or data destinations, and determine how these changes may affect the flow of data.', 'Auditing and Reporting: The ability to capture, store, and report on changes made to data and metadata, to support compliance and risk management initiatives.', 'Integration with Other Tools: Integration with other data management and governance tools, such as data quality tools, data dictionaries, and metadata repositories, to provide a complete picture of the data landscape.', 'User-Friendly Interface: A user-friendly interface that makes it easy for users to understand the flow of data and trace data lineage, even for non-technical users.', 'Scalability: The ability to scale to meet the needs of large organizations with complex data landscapes.', 'Security: Measures to ensure the security of sensitive data, such as encryption, access controls, and data masking.', 'Reporting and Analytics: The ability to generate reports and perform analytics on data lineage information to support decision making and continuous improvement initiatives.'],
                "role":"Machine Learning Engineer",
                "org": "Virtusa",
                "tech": ["python","neo4j",'airflow',"AWS"],
                "Links":"",
                "category":"Others",
                "year":2018
                
        },
]
    
    